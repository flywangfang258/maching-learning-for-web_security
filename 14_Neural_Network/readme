浅层学习：模型的识别能力更多的取决于特征选取的有效性。基于人工定义的特征进行分类检测
浅层学习使用时，需要花费至少一半的时间在数据清洗和特征提取上，即特征工程。

神经网络：多个输入参数，分别具有各自的权重，经过激励函数的处理后，得到输出，
        输出可以再对接下一级的神经网络的输入，从而组成更复杂的神经网络。

Scikit-Learn中的神经网络实现都是使用反向传播算法。


clf = MLPClassifier(solver='sgd',activation = 'identity',max_iter = 10,alpha = 1e-5,hidden_layer_sizes = (100,50),random_state = 1,verbose = True)

参数说明: 
1. hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。 
2. activation :激活函数,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’},
默认relu - identity：f(x) = x - logistic：其实就是sigmod,f(x) = 1 / (1 + exp(-x)). - tanh：f(x) = tanh(x). - relu：f(x) = max(0, x) 
3. solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重 
- lbfgs：quasi-Newton方法的优化器 
- sgd：随机梯度下降 
- adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 
注意：默认solver ‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。 
4. alpha :float,可选的，默认0.0001,正则化项参数 
5. batch_size : int , 可选的，默认’auto’,随机优化的minibatches的大小batch_size=min(200,n_samples)，如果solver是’lbfgs’，分类器将不使用minibatch 
6. learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant 
- ‘constant’: 有’learning_rate_init’给定的恒定学习率 
- ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t) 
- ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. 
7. power_t: double, 可选, default 0.5，只有solver=’sgd’时使用，是逆扩展学习率的指数.当learning_rate=’invscaling’，用来更新有效学习率。
 8. max_iter: int，可选，默认200，最大迭代次数。
 9. random_state:int 或RandomState，可选，默认None，随机数生成器的状态或种子。 
10. shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。 
11. tol：float, 可选，默认1e-4，优化的容忍度 
12. learning_rate_int:double,可选，默认0.001，初始学习率，控制更新权重的补偿，只有当solver=’sgd’ 或’adam’时使用。 
14. verbose : bool, 可选, 默认False,是否将过程打印到stdout 
15. warm_start : bool, 可选, 默认False,当设置成True，使用之前的解决方法作为初始拟合，否则释放之前的解决方法。 
16. momentum : float, 默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用. 
17. nesterovs_momentum : boolean, 默认True, Whether to use Nesterov’s momentum. 只有solver=’sgd’并且momentum > 0使用. 
18. early_stopping : bool, 默认False,只有solver=’sgd’或者’adam’时有效,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续迭代改善，低于tol时终止训练。 
19. validation_fraction : float, 可选, 默认 0.1,用作早期停止验证的预留训练数据集的比例，早0-1之间，只当early_stopping=True有用 
20. beta_1 : float, 可选, 默认0.9，只有solver=’adam’时使用，估计一阶矩向量的指数衰减速率，[0,1)之间 
21. beta_2 : float, 可选, 默认0.999,只有solver=’adam’时使用估计二阶矩向量的指数衰减速率[0,1)之间 
22. epsilon : float, 可选, 默认1e-8,只有solver=’adam’时使用数值稳定值。 

属性说明： 
- classes_:每个输出的类标签 
- loss_:损失函数计算出来的当前损失值 
- coefs_:列表中的第i个元素表示i层的权重矩阵 
- intercepts_:列表中第i个元素代表i+1层的偏差向量 
- n_iter_ ：迭代次数 
- n_layers_:层数 
- n_outputs_:输出的个数 
- out_activation_:输出激活函数的名称。 

方法说明： 
- fit(X,y):拟合 
- get_params([deep]):获取参数 
- predict(X):使用MLP进行预测 
- predic_log_proba(X):返回对数概率估计 
- predic_proba(X)：概率估计 
- score(X,y[,sample_weight]):返回给定测试数据和标签上的平均准确度 
-set_params(**params):设置参数。



一个epoch指的是使用全部数据集进行一次训练。进行训练时一个epoch可能更新了若干次参数。epoch_num为指定的epoch次数。
# 一个step或一次iteration指的是更新一次参数，每次更新使用数据集中的batch_size个数据。
# 注意: 使用相同的数据集，epoch也相同时，参数更新此时不一定是相同的，这时候会取决于batch_size。
# iteration或step的总数为(数据总数 / batch_size + 1) * epoch_num
# 每个epoch都会进行shuffle，对要输入的数据进行重新排序，分成不同的batch。
